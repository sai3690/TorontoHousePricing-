# -*- coding: utf-8 -*-
"""TorontoHousePred-||.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GnjazoTSTwduZ3f0VetfuEM8VAKTBQNA
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

#from google.colab import drive
#drive.mount('/content/gdrive')

#from google.colab import files
#uploaded = files.upload()

"""# Loading the data after processing the GeoRecord of latitude and longitude"""

dataset2 = pd.read_excel ("HouseFinal.xlsx")

"""Checking the head of dataframe"""

dataset2.head()

dataset2.isnull().sum()

"""There are still some null values that needs to processed"""

dataset2.describe()

"""Describe the dataset"""

dataset2.drop('Index', axis=1,inplace= True)

dataset2.dropna()

dataset2.shape

dataset2.isnull().sum()

dataset2.dropna(how='all', inplace=True)

dataset2.isnull().sum()

import plotly.express as px


fig = px.scatter_geo(dataset2,lat='Latitude',lon='Longitude', hover_name="final_price")
fig.update_layout(title = 'S', title_x=0.5)
fig.show()

dataset2.corr()

"""Checking the corelation between different data points"""

dataset2['mls'].value_counts()

"""# Categorical data- column Type

There are various ways of dealing with categorical data, Here we are using One hot encoding.
"""

import sklearn
from sklearn.preprocessing import OneHotEncoder
one = OneHotEncoder()

type_array=one.fit_transform(dataset2[['type']]).toarray()

one.categories_

type_labels = one.categories_

type_labels = np.array(type_labels).ravel()

print(type_labels)

"""

```
# This is formatted as code
```

After applying oneHot encoding data looks like this:"""

pd.DataFrame(type_array, columns =type_labels )

dataset2type = pd.DataFrame(type_array, columns =type_labels )

datasetnew = pd.concat([dataset2, dataset2type], axis=1)

datasetnew.shape

datasetnew.dropna(
    axis=0,
    how='any',
    thresh=None,
    subset=None,
    inplace=True
)
datasetnew.isnull().sum()

"""Now concating two data frames to process dataset new."""

datasetnew.head()

"""# Data visualization-pairplot"""

sns.pairplot(datasetnew)

datasetnew['description'] = datasetnew['description'].str.lower()
datasetnew['description'] = datasetnew['description'].str.strip()

import warnings
warnings.filterwarnings("ignore")
from subprocess import check_output

import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls
import os
import gc

import re
from nltk.corpus import stopwords

from nltk.stem import PorterStemmer
from bs4 import BeautifulSoup

import re
from nltk.corpus import stopwords

from nltk.stem import PorterStemmer
from bs4 import BeautifulSoup
import re
from nltk.corpus import stopwords

"""Word cloud of description column"""

from wordcloud import WordCloud


# Create a list of word
text = " ".join(review for review in datasetnew.description.astype(str))

# Create the wordcloud object
wordcloud = WordCloud(width=480, height=480, margin=0).generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
plt.figure( figsize=(90,40))
plt.show()

from sklearn.feature_extraction.text import CountVectorizer

# Initialize the vectorizer
vectorizer = CountVectorizer()

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

datasetnew["description"] = datasetnew["description"].str.lower()
datasetnew["description"] = datasetnew["description"].str.replace('[^\w\s]','')

# Remove numbers
datasetnew["description"] = datasetnew["description"].str.replace('\d','')

import nltk
nltk.download('stopwords')

# Remove stopwords
stop_words = set(stopwords.words("english"))
datasetnew["description"] = datasetnew["description"].apply(lambda x: " ".join(x for x in x.split() if x not in stop_words))

nltk.download('omw-1.4')
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
datasetnew["description"] = datasetnew["description"].apply(lambda x: " ".join([lemmatizer.lemmatize(word) for word in x.split()]))

vectorizer.fit(datasetnew["description"])

# Transform the text data into a numerical matrix
T = vectorizer.transform(datasetnew["description"])

print(T)

"""Box Plot:"""

np.random.seed(1234)

boxplot = datasetnew.boxplot(column=['final_price', 'bathrooms','parking','new_bedroom','new_sqft'])

"""Histogram:"""

hist = datasetnew.hist(column=['final_price', 'bathrooms','parking','new_bedroom','new_sqft'])

datasetnew['description'].replace('', np.nan, inplace=True)
dataset2.isnull().sum()

datasetnew.dropna(
    axis=0,
    how='any',
    thresh=None,
    subset=None,
    inplace=True
)
datasetnew.isnull().sum()

"""# ** Modeling**

Test-train split
"""

from sklearn.model_selection import train_test_split
datasetnew.columns
datasetnew.head(5)

"""Selecting X and Y variables for Target """

X = datasetnew[[ 'bathrooms', 'parking',
         'new_bedroom', 'new_sqft',
      'Latitude', 'Longitude', 'Att/Row/Twnhouse', 'Co-Op Apt',
       'Co-Ownership Apt', 'Comm Element Condo', 'Condo Apt',
       'Condo Townhouse', 'Det Condo', 'Detached', 'Duplex', 'Fourplex',
       'Leasehold Condo', 'Multiplex', 'Semi-Detached', 'Triplex']]

Y = datasetnew[['final_price']]

## Standardize the dataset
#from sklearn.preprocessing import StandardScaler
#scaler=StandardScaler()

#X_std = scaler.fit_transform(X)

#Y_std = scaler.fit_transform(Y)

#X

"""Data Oints after Split of Train and Test"""

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=1)
X_train.shape

Y_train.shape

"""**Linear regression**"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error

param_grid = {'normalize': [True, False]}

lr = LinearRegression()

# Use GridSearchCV to perform hyperparameter tuning
grid_search = GridSearchCV(lr, param_grid, cv=5)
grid_search.fit(X_train, Y_train)

# Print the best parameters found
print("Best parameters found: ", grid_search.best_params_)

# Predict the values for the test set
Y_pred = grid_search.predict(X_test)

# Calculate the mean squared error and mean absolute error
mse = mean_squared_error(Y_test, Y_pred)
mae = mean_absolute_error(Y_test, Y_pred)

# Print the results
print("Mean squared error: ", mse)
print("Mean absolute error: ", mae)

plt.scatter(Y_test,reg_pred)
#Plot scatter plot for the prediction

#residules (Errors)
residules = (Y_test - reg_pred)
residules

sns.displot(residules,kind='kde')

"""As we are getting a normal distribution, We can say this modle is performing well"""

plt.scatter(reg_pred,residules)
#no distribution is observed

from sklearn.metrics import r2_score
score = r2_score(Y_test,reg_pred)
print(score)

"""Adjusted R2 = 1 â€“ [(1-R2)*(n-1)/(n-k-1)]

where:

R2: The R2 of the model n: The number of observations k: The number of predictor variables

"""

#display adjusted R-squared

1 - (1-score)*(len(Y_test)-1)/(len(Y_test)-X_test.shape[1]-1)

"""# SVM"""

from sklearn.svm import SVR

# Create an instance of the SVR model
svr = SVR()

# Define the hyperparameters to tune
parameters = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
              'C': [0.1, 1, 10, 100],
              'epsilon': [0.1, 0.2, 0.3, 0.4, 0.5]}

# Use GridSearchCV to perform a grid search with cross-validation
grid_search = GridSearchCV(svr, parameters, cv=5)
grid_search.fit(X_train, Y_train.values.ravel())

# Get the best hyperparameters from the grid search
print("Best parameters:", grid_search.best_params_)

# Make predictions using the best hyperparameters
Y_pred = grid_search.predict(X_test)

Y_test = Y_test.values.ravel()

# Calculate the residual and R^2 score
residual = Y_test - Y_pred
r2 = r2_score(Y_test, Y_pred)

print("Residual:", residual)
print("R^2:", r2)

"""# Decision Tree"""

from sklearn.tree import DecisionTreeRegressor

# Create an instance of the DecisionTreeRegressor model
dtr = DecisionTreeRegressor()

# Define the hyperparameters to tune
parameters = {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
              'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]}

# Use GridSearchCV to perform a grid search with cross-validation
grid_search = GridSearchCV(dtr, parameters, cv=5)
grid_search.fit(X_train, Y_train)

# Get the best hyperparameters from the grid search
print("Best parameters:", grid_search.best_params_)

# Make predictions using the best hyperparameters
Y_pred = grid_search.predict(X_test)

# Calculate the residual and R^2 score
residual = Y_test - Y_pred
r2 = r2_score(Y_test, Y_pred)

print("Residual:", residual)
print("R^2:", r2)

"""Random forest **bold text**"""

from sklearn.ensemble import RandomForestRegressor


# Create an instance of the RandomForestRegressor model
rfr = RandomForestRegressor()

# Define the hyperparameters to tune
parameters = {'n_estimators': [10, 50, 100, 200, 300, 400, 500],
              'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
              'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]}

# Use GridSearchCV to perform a grid search with cross-validation
grid_search = GridSearchCV(rfr, parameters, cv=5)
grid_search.fit(X_train, Y_train)

# Get the best hyperparameters from the grid search
print("Best parameters:", grid_search.best_params_)

# Make predictions using the best hyperparameters
Y_pred = grid_search.predict(X_test)

# Calculate the residual and R^2 score
residual = Y_test - Y_pred
r2 = r2_score(Y_test, Y_pred)

print("Residual:", residual)
print("R^2:", r2)

"""# New Section

**KNN **
"""

#from sklearn import svm

#svm.SVC(kernel='linear', gamma = 'auto',C=2)
#classifire_svm = svm.SVC(kernel='linear', gamma = 'auto',C=2)
#classifire_svm.fit(X_train,Y_train)

from sklearn.neighbors import KNeighborsClassifier

classifire_KNN = KNeighborsClassifier(n_neighbors=2)
classifire_KNN.fit(X_train,Y_train)

knn_pred = classifire_KNN.predict(X_test)
knn_pred

plt.scatter(Y_test,knn_pred)

from sklearn.metrics import r2_score
score_knn = r2_score(Y_test,knn_pred)
print(score_knn)

"""**XGB**"""

#from xgboost import XGBClassifier

# fit model no training data
#model = XGBClassifier()
#xg_pred=model.fit(X_train, Y_train)

#xg_pr = xg_pred.predict(X_test)
#xg_pr

#plt.scatter(Y_test,xg_pr)

#from sklearn.metrics import r2_score
#score_xg = r2_score(Y_test,xg_pr)
#print(score_xg)

print(X.shape)

"""[ 'bathrooms', 'parking',
         'new_bedroom', 'new_sqft',
      'Latitude', 'Longitude', 'Att/Row/Twnhouse', 'Co-Op Apt',
       'Co-Ownership Apt', 'Comm Element Condo', 'Condo Apt',
       'Condo Townhouse', 'Det Condo', 'Detached', 'Duplex', 'Fourplex',
       'Leasehold Condo', 'Multiplex', 'Semi-Detached', 'Triplex']
"""

feature_values = np.array([[1, 4, 3, 2500, 43.564230, -79.650760, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,1,0]])

# Make a prediction using the model
prediction = grid_search.predict(feature_values)

# Print the prediction
print(int(prediction[0]))

import pickle

filename = 'random_forest_model.pkl'
pickle.dump(grid_search, open(filename, 'wb'))

pickled_model=pickle.load(open(filename,'rb'))

pickled_model.predict(feature_values)